{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QiQ1Sj_X6B"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOKJp06-_X6C"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWMDmr7_X6D"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iB-7bwR_X6D"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0KRjjr3D_X6D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 1. Uninstall EVERYTHING related to Unsloth and its core dependencies for a clean slate.\n",
        "# This is crucial to avoid conflicts and ensure fresh installs.\n",
        "!pip uninstall -y unsloth unsloth_zoo transformers bitsandbytes accelerate peft trl triton xformers torch torchvision torchaudio numpy packaging pyyaml regex requests safetensors tokenizers tqdm\n",
        "\n",
        "# 2. Install PyTorch and its domain libraries from the official wheel index.\n",
        "# These versions were identified as working with your Colab environment.\n",
        "!pip install --no-cache-dir \\\n",
        "    torch==2.6.0+cu124 \\\n",
        "    torchvision==0.21.0+cu124 \\\n",
        "    torchaudio==2.6.0+cu124 \\\n",
        "    --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# 3. Install the LATEST Unsloth from its main branch and pin transformers to your working version.\n",
        "# The [colab-new] extra pulls in necessary Colab-specific deps.\n",
        "# By installing from git, we get the very latest Unsloth which should be compatible\n",
        "# with the latest stable transformers versions like 4.53.1.\n",
        "!pip install --no-cache-dir \\\n",
        "    transformers==4.53.1 \\\n",
        "    \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# 4. Install other key dependencies, using the exact versions you found.\n",
        "# These are essential for Unsloth's functionality and prevent further conflicts.\n",
        "!pip install --no-cache-dir \\\n",
        "    bitsandbytes==0.46.1 \\\n",
        "    accelerate==0.31.0 \\\n",
        "    peft==0.15.2 \\\n",
        "    trl==0.8.6 \\\n",
        "    triton==3.2.0 \\\n",
        "    xformers==0.0.29.post3 \\\n",
        "    datasets==2.19.1 \\\n",
        "    huggingface_hub==0.23.4 \\\n",
        "    sentencepiece==0.2.0\n",
        "    # Add other common dependencies if they are missing after the above installs,\n",
        "    # e.g., datasets, huggingface_hub, sentencepiece, etc.\n",
        "    # You can get the exact versions by running:\n",
        "    # !pip show datasets huggingface_hub sentencepiece\n",
        "# IMPORTANT: After running this cell, you MUST restart the Colab runtime!\n",
        "# Go to \"Runtime\" -> \"Restart session\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvcwcvlaBwOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show unsloth unsloth_zoo transformers bitsandbytes accelerate peft trl triton xformers torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "LN-eAAUv5GpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6005457-1dfb-4721-fb82-12d8c41a5f75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: xformers\u001b[0m\u001b[33m\n",
            "\u001b[0mName: unsloth\n",
            "Version: 2025.7.1\n",
            "Summary: 2-5X faster LLM finetuning\n",
            "Home-page: http://www.unsloth.ai\n",
            "Author: Unsloth AI team\n",
            "Author-email: info@unsloth.ai\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "---\n",
            "Name: unsloth_zoo\n",
            "Version: 2025.7.1\n",
            "Summary: Utils for Unsloth\n",
            "Home-page: http://www.unsloth.ai\n",
            "Author: Unsloth AI team\n",
            "Author-email: info@unsloth.ai\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, cut_cross_entropy, datasets, hf_transfer, huggingface_hub, msgspec, numpy, packaging, peft, pillow, protobuf, psutil, regex, sentencepiece, torch, tqdm, transformers, triton, trl, tyro, wheel\n",
            "Required-by: \n",
            "---\n",
            "Name: transformers\n",
            "Version: 4.53.1\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers, trl, unsloth_zoo\n",
            "---\n",
            "Name: bitsandbytes\n",
            "Version: 0.46.1\n",
            "Summary: k-bit optimizers and matrix multiplication routines.\n",
            "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
            "Author: \n",
            "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
            "License: MIT License\n",
            "\n",
            "Copyright (c) Facebook, Inc. and its affiliates.\n",
            "\n",
            "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "of this software and associated documentation files (the \"Software\"), to deal\n",
            "in the Software without restriction, including without limitation the rights\n",
            "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "copies of the Software, and to permit persons to whom the Software is\n",
            "furnished to do so, subject to the following conditions:\n",
            "\n",
            "The above copyright notice and this permission notice shall be included in all\n",
            "copies or substantial portions of the Software.\n",
            "\n",
            "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
            "SOFTWARE.\n",
            "\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, torch\n",
            "Required-by: \n",
            "---\n",
            "Name: accelerate\n",
            "Version: 1.8.1\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: peft, trl, unsloth_zoo\n",
            "---\n",
            "Name: peft\n",
            "Version: 0.16.0\n",
            "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
            "Home-page: https://github.com/huggingface/peft\n",
            "Author: The HuggingFace team\n",
            "Author-email: benjamin@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
            "Required-by: unsloth_zoo\n",
            "---\n",
            "Name: trl\n",
            "Version: 0.19.1\n",
            "Summary: Train transformer language models with reinforcement learning.\n",
            "Home-page: https://github.com/huggingface/trl\n",
            "Author: Leandro von Werra\n",
            "Author-email: leandro.vonwerra@gmail.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, datasets, transformers\n",
            "Required-by: unsloth_zoo\n",
            "---\n",
            "Name: triton\n",
            "Version: 3.2.0\n",
            "Summary: A language and compiler for custom Deep Learning operations\n",
            "Home-page: https://github.com/triton-lang/triton/\n",
            "Author: Philippe Tillet\n",
            "Author-email: phil@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: cut-cross-entropy, torch, unsloth_zoo\n",
            "---\n",
            "Name: torch\n",
            "Version: 2.6.0+cu124\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, bitsandbytes, cut-cross-entropy, fastai, peft, sentence-transformers, timm, torchaudio, torchdata, torchvision, unsloth_zoo\n",
            "---\n",
            "Name: torchvision\n",
            "Version: 0.21.0+cu124\n",
            "Summary: image and video datasets and models for torch deep learning\n",
            "Home-page: https://github.com/pytorch/vision\n",
            "Author: PyTorch Core Team\n",
            "Author-email: soumith@pytorch.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, pillow, torch\n",
            "Required-by: fastai, timm\n",
            "---\n",
            "Name: torchaudio\n",
            "Version: 2.6.0+cu124\n",
            "Summary: An audio package for PyTorch\n",
            "Home-page: https://github.com/pytorch/audio\n",
            "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
            "Author-email: soumith@pytorch.org\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFOEZbP7ONMs"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "76022979-8283-4e19-a7a7-7a2de2608383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.1: Fast Gemma3 patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, processor = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/gemma-3-4b-pt\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient fine-tuning, allowing us to train only 1% of all model parameters efficiently.\n",
        "\n",
        "**[NEW]** We also support fine-tuning only the vision component, only the language component, or both. Additionally, you can choose to fine-tune the attention modules, the MLP layers, or both!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,                           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,                  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,               # We support rank stabilized LoRA\n",
        "    loftq_config = None,               # And LoftQ\n",
        "    target_modules = \"all-linear\",    # Optional now! Can specify a list if needed\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We'll use a sampled dataset of handwritten math formulas. The objective is to convert these images into a computer-readable format—specifically LaTeX—so they can be rendered. This is particularly useful for complex expressions.\n",
        "\n",
        "You can access the dataset [here](https://huggingface.co/datasets/unsloth/LaTeX_OCR). The full dataset is [here](https://huggingface.co/datasets/linxy/LaTeX_OCR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"unsloth/LaTeX_OCR\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1W2Qhsz6rUT"
      },
      "source": [
        "Let's take an overview of the dataset. We'll examine the second image and its corresponding caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfcSGwIb6p_R"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOLWY2936t1n"
      },
      "outputs": [],
      "source": [
        "dataset[2][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXjfJr4W6z8P"
      },
      "outputs": [],
      "source": [
        "dataset[2][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKHxfZua1CrS"
      },
      "source": [
        "We can also render LaTeX directly in the browser!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPopsxAC1CrS"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "latex = dataset[3][\"text\"]\n",
        "display(Math(latex))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision fine-tuning tasks should follow this format:\n",
        "\n",
        "```python\n",
        "[\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "            {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "            {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "            ],\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-9u-OD6_gE"
      },
      "source": [
        "Let's convert the dataset into the \"correct\" format for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFW2qXIr7Ezy"
      },
      "outputs": [],
      "source": [
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "The first example is now structured like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "converted_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS55FC1f1CrS"
      },
      "source": [
        "Lets take the Gemma 3 instruction chat template and use it in our base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEzvL7Sm1CrS"
      },
      "outputs": [],
      "source": [
        "from unsloth import get_chat_template\n",
        "\n",
        "processor = get_chat_template(\n",
        "    processor,\n",
        "    \"gemma-3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FecKS-dA82f5"
      },
      "source": [
        "Before fine-tuning, let us evaluate the base model's performance. We do not expect strong results, as it has not encountered this chat template before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcat4UxA81vr"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[2][\"image\"]\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
        "    }\n",
        "]\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
        "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeAiMlQ71CrS"
      },
      "source": [
        "You can see it's absolutely terrible! It doesn't follow instructions at all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=converted_dataset,\n",
        "    processing_class=processor.tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, processor),\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        gradient_checkpointing = True,\n",
        "\n",
        "        # use reentrant checkpointing\n",
        "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
        "        max_grad_norm = 0.3,              # max gradient norm based on QLoRA paper\n",
        "        warmup_ratio = 0.03,\n",
        "        max_steps = 30,\n",
        "        #num_train_epochs = 2,          # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        save_strategy=\"steps\",\n",
        "        optim = \"adamw_torch_fused\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",             # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc = 2,\n",
        "        max_seq_length = 2048,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can modify the instruction and input—just leave the output blank.\n",
        "\n",
        "We'll use the best hyperparameters for inference on Gemma: `top_p=0.95`, `top_k=64`, and `temperature=1.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[10][\"image\"]\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
        "    }\n",
        "]\n",
        "\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
        "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, use Hugging Face’s `push_to_hub` for online saving, or `save_pretrained` for local storage.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "processor.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "\n",
        "    model, processor = FastVisionModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=True,  # Set to False for 16bit LoRA\n",
        "    )\n",
        "    FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "sample = dataset[1]\n",
        "image = sample[\"image\"].convert(\"RGB\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": sample[\"text\"],\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(processor.tokenizer, skip_prompt=True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", processor,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", processor, token = \"PUT_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NawANgOU_X6S"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}